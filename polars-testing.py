# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ieEgevuv5-v6BEpuaDyUOM2NiERMPV-3

### Polars GPU Engine
[Polars](https://github.com/pola-rs/polars) is a DataFrame interface on top of an OLAP Query Engine implemented in Rust using Apache Arrow Columnar Format as the memory model.

The goal of Polars is to provide a lightning fast DataFrame library that :-

*  Utilizes all available cores on your machine.
*  Optimizes queries to reduce unneeded work/memory allocations.
*  Handles datasets much larger than your available RAM, etc.
"""

!nvidia-smi | head

"""#### Setup"""

!pip install polars==1.5
!pip install "cudf_polars_cu12-24.8.0a433-py3-none-any.whl" --extra-index-url=https://pypi.anaconda.org/rapidsai-wheels-nightly/simple

#Importing other dependencies
!pip install hvplot jupyter_bokeh holoviews==1.19 pynvml

"""### Downloading the data

Dataset is from [Kaggle](https://www.kaggle.com/datasets/conorsully1/simulated-transactions) and using about just a fraction of data roughly around 20 gigs.

"""

import pynvml
pynvml.nvmlInit()
mem=pynvml.nvmlDeviceGetMemoryInfo(pynvml.nvmlDeviceGetHandleByIndex(0))
mem= mem.total/1e9

if mem<24:
  !wget https://storage.googleapis.com/rapidsai/polars-demo/transactions-t4-20.parquet -O transactions.parquet
else:
  !wget https://storage.googleapis.com/rapidsai/polars-demo/transactions.parquet -O transactions.parquet


!wget https://storage.googleapis.com/rapidsai/polars-demo/rainfall_data_2010_2020.csv

# Using polars to read Parquet file

import polars as pl
import hvplot.polars
from polars.testing import assert_frame_equal

pl.__version__

transactions=pl.scan_parquet("transactions.parquet")

transactions.collect_schema()

transactions.head(5).collect()

"""## COMPARING CPU with GPU

### 1
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# transactions.select(pl.col("AMOUNT").sum()).collect()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# transactions.select(pl.col("AMOUNT").sum()).collect(engine="gpu")

#Using pl.GPTEngine with CPU fallback for unsupported operations
gpu_engine=pl.GPUEngine(
    device=0,
    raise_on_fail=True,
)

"""### 2

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# res_cpu=(
#     transactions
#     .group_by("CUST_ID")
#     .agg(pl.col("AMOUNT").sum())
#     .sort(by="AMOUNT",descending=True)
#     .head()
#     .collect()
# )
# res_cpu
# 
#

# Commented out IPython magic to ensure Python compatibility.
# ## Same Complex query exection with GPU
# 
# %%time
# 
# res_gpu = (
#     transactions
#     .group_by("CUST_ID")
#     .agg(pl.col("AMOUNT").sum())
#     .sort(by="AMOUNT",descending=True)
#     .head()
#     .collect(engine=gpu_engine)
# )
# res_gpu

"""### 3"""

# Commented out IPython magic to ensure Python compatibility.
# # Customer having Largest transaction
# %%time
# 
# res_cpu = (
#     transactions
#     .group_by("CUST_ID")
#     .agg(pl.col("AMOUNT").max().alias("max_amount"))
#     .sort(by="max_amount", descending=True)
#     .head()
#     .collect()
# )
# res_cpu

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# res_gpu= (
#     transactions
#     .group_by("CUST_ID")
#     .agg(pl.col("AMOUNT").max().alias("max_amount"))
#     .sort(by="max_amount", descending=True)
#     .head()
#     .collect(engine=gpu_engine)
# )
# res_gpu

"""## Profiling, why polars gpu isn't always the winner"""

res, prof = (
    transactions
    .filter(pl.col("CUST_ID") == "CIP0I11MG2")
    .select(pl.col("AMOUNT").max())
    .profile()
)

prof.with_columns(
    ((pl.col("end") - pl.col("start")) / pl.col("end").max() * 100)
    .alias("pct_time_spent")
)

"""### While reading the data there is less ground to cover for gpu-acceleration as 99% of time is spent on IO

### 4
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# res = (
#     transactions
#     .group_by(["EXP_TYPE", "YEAR", "MONTH"])
#     .agg(pl.mean("AMOUNT"))
#     .sort(["EXP_TYPE", "YEAR", "MONTH"])
#     .collect()
# )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# res = (
#     transactions
#     .group_by(["EXP_TYPE", "YEAR", "MONTH"])
#     .agg(pl.mean("AMOUNT"))
#     .sort(["EXP_TYPE", "YEAR", "MONTH"])
#     .collect(engine=gpu_engine)
# )

"""### We can see strong usage of gpu-acceleration as this operation was more compute intensive as compared to IO bound operation

### Handling more than 1 dataset
"""

name=['Location','Rainfall(inches)','Date','YEAR','MONTH','DAY']

weather=pl.scan_csv("rainfall_data_2010_2020.csv",new_columns=name)
weather.head().collect()

weather_cleaned=(
    weather
    .with_columns(pl.col("Date").cast(pl.Utf8).str.strptime(pl.Date(), "%Y%m%d"))
    .collect()
)

# Commented out IPython magic to ensure Python compatibility.
# ## Joining the dataset and executing query to determine amount of rainfall
# 
# %%time
# 
# (
#     transactions
#     .join(
#         other=weather_cleaned.lazy(),
#         left_on="DATE",
#         right_on="Date",
#         how="inner"
#     )
#     .group_by(["EXP_TYPE","DATE"])
#     .agg(pl.mean("Rainfall(inches)"))
#     .sort(["DATE","EXP_TYPE","Rainfall(inches)"])
#     .head()
#     .collect()
# )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# (
#     transactions
#     .join(
#         other=weather_cleaned.lazy(),
#         left_on="DATE",
#         right_on="Date",
#         how="inner"
#     )
#     .group_by(["EXP_TYPE","DATE"])
#     .agg(pl.mean("Rainfall(inches)"))
#     .sort(["DATE","EXP_TYPE","Rainfall(inches)"])
#     .head()
#     .collect(engine=gpu_engine)
# )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# (
#     transactions
#     .join(
#         other=weather_cleaned.lazy(),
#         left_on="DATE",
#         right_on="Date",
#         how="inner"
#     )
#     .group_by(["EXP_TYPE","DATE"])
#     .agg(pl.mean("Rainfall(inches)"),pl.sum("AMOUNT"))
#     .sort(["DATE","EXP_TYPE","Rainfall(inches)"])
#     .head()
#     .collect(engine=gpu_engine)
# )

